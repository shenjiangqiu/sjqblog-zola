<?xml version="1.0" encoding="UTF-8"?>
<rss xmlns:atom="http://www.w3.org/2005/Atom" version="2.0">
    <channel>
      <title> - accelerator</title>
      <link>https://shenjiangqiu.github.io/sjqblog-zola/</link>
      <description></description>
      <generator>Zola</generator>
      <language>en</language>
      <atom:link href="https://shenjiangqiu.github.io/sjqblog-zola/tags/accelerator/rss.xml" rel="self" type="application/rss+xml"/>
      <lastBuildDate>Thu, 05 Dec 2024 00:00:00 +0000</lastBuildDate>
      <item>
          <title>
    
    
    soft-instinfer
</title>
          <pubDate>Thu, 05 Dec 2024 00:00:00 +0000</pubDate>
          <author>Unknown</author>
          <link>https://shenjiangqiu.github.io/sjqblog-zola/posts/daily/soft-instinfer/</link>
          <guid>https://shenjiangqiu.github.io/sjqblog-zola/posts/daily/soft-instinfer/</guid>
          <description>&lt;h2 id=&quot;sofa-a-compute-memory-optimized-sparsity-accelerator-via-cross-stage-coordinated-tiling&quot;&gt;&lt;a href=&quot;&#x2F;pdf&#x2F;sofa.pdf&quot;&gt;SOFA: A Compute-Memory Optimized Sparsity Accelerator via Cross-Stage Coordinated Tiling&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
&lt;h3 id=&quot;motivation&quot;&gt;Motivation&lt;&#x2F;h3&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;Bottlenecks in Transformers: As sequence lengths increase, the attention module becomes the dominant bottleneck in Transformer-based inference, surpassing the feed-forward network (FFN).&lt;&#x2F;p&gt;
&lt;&#x2F;li&gt;
&lt;li&gt;
&lt;p&gt;Limitations of Existing Sparse Accelerators:&lt;&#x2F;p&gt;
&lt;ul&gt;
&lt;li&gt;Current accelerators struggle with the demands of high-throughput and large-scale token parallelism.&lt;&#x2F;li&gt;
&lt;li&gt;They focus on stage-wise optimization, leading to inefficiencies in memory access and computation.&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;proposed-solution-sofa&quot;&gt;Proposed Solution: SOFA&lt;&#x2F;h3&gt;
&lt;p&gt;SOFA addresses the above challenges using a cross-stage compute-memory optimized approach:&lt;&#x2F;p&gt;
&lt;ol&gt;
&lt;li&gt;Differential Leading Zero Summation (DLZS): A novel multiplier-free, log-based computation paradigm to predict sparsity efficiently, significantly reducing overhead during the pre-compute stage.&lt;&#x2F;li&gt;
&lt;li&gt;Sphere-Search Aided Distributed Sorting (SADS): Divides sequences into sub-segments for lightweight sorting, minimizing comparisons and enabling fine-grained tiling.&lt;&#x2F;li&gt;
&lt;li&gt;Sorted-Updating FlashAttention (SU-FA): Reduces the computational cost of softmax and attention updates by leveraging cross-stage tiling and distributed sorting information.&lt;&#x2F;li&gt;
&lt;&#x2F;ol&gt;
&lt;h3 id=&quot;flashattention&quot;&gt;flashattention&lt;&#x2F;h3&gt;
&lt;!-- ![flashattention](&#x2F;img&#x2F;flashatt.png) --&gt;
&lt;img src=&quot;https:&#x2F;&#x2F;shenjiangqiu.github.io&#x2F;sjqblog-zola&#x2F;img&#x2F;flashatt.png&quot; alt=&quot;alt&quot;&#x2F;&gt;
&lt;!-- ![flashattention2](&#x2F;img&#x2F;falshatt_paper.png) --&gt;
&lt;img src=&quot;https:&#x2F;&#x2F;shenjiangqiu.github.io&#x2F;sjqblog-zola&#x2F;img&#x2F;falshatt_paper.png&quot; alt=&quot;alt&quot;&#x2F;&gt;
&lt;ul&gt;
&lt;li&gt;in line5, it update the max of the scores of m_i&lt;&#x2F;li&gt;
&lt;li&gt;in line7,&lt;&#x2F;li&gt;
&lt;&#x2F;ul&gt;
&lt;h3 id=&quot;analysis&quot;&gt;analysis&lt;&#x2F;h3&gt;
&lt;h2 id=&quot;&quot;&gt;&lt;a href=&quot;https:&#x2F;&#x2F;shenjiangqiu.github.io&#x2F;sjqblog-zola&#x2F;pdf&#x2F;instinfer.pdf&quot; target=&quot;_blank&quot;&gt;
    instinfr
&lt;&#x2F;a&gt;&lt;&#x2F;h2&gt;
</description>
      </item>
    </channel>
</rss>