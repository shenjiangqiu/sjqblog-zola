+++
[taxonomies]
tags = ["GPT","transformers","accelerators"]
authors = ["Jiangqiu Shen"]
+++

this post summarizes the recent progress of GPT-3 and its accelerators.

the papers included:

- Att is all you need [paper](/static/pdf/attAllYouNeed.pdf)
- Orca, [paper](/pdf/osdi22-yu.pdf)
- Fast Distributed Inference Serving for Large Language Models, [paper](/static/pdf/FastDist.pdf)
- DeepSpeed, [paper](/static/pdf/deepspeed.pdf)
- FlexGen High-throughput Generative Inference of Large Language Models with a Single GPU;   [paper](/static/pdf/flashgen.pdf)
- AttentionFlash, [paper](/static/pdf/FlashAtt.pdf)
- LightSeq2, [paper](/static/pdf/LightSeq2.pdf)
- FLAT.[paper](/static/pdf/Flat.pdf)
<!-- more -->

## in memory
- TransPIM: A Memory-based Acceleration via Software-Hardware Co-Design for Transformer [paper](/static/pdf/TransPIM.pdf)
- Unleashing the Potential of PIM: Accelerating Large Batched Inference of Transformer-Based Generative Models [paper](/static/pdf/Unleashing.pdf)
- FACT [paper](/static/pdf/FACT.pdf)
- DFX [paper](/static/pdf/DFX.pdf)

## Attation is all you need
