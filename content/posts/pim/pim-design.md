+++
[taxonomies]
tags = ["research", "pim","gearbox","fulcrum","spmv"]

+++
the post illustrate the current design of Fulcrum and gearbox, and show the challenges of SPMSPM design.

the related papers:
- Gearbox: [paper](/pdf/Gearbox.pdf)
- Fulcrum: [paper](/pdf/Fulcrum_A_Simplified_Control_and_Access_Mechanism_Toward_Flexible_and_Practical_In-Situ_Accelerators.pdf)
- Chopper: [paper](/pdf/CHOPPER-HPCA-23.pdf)



design:
- async remote task
- buffers and nocs
- traffic flow control




the original discuss paper scan:
- [scans 1](/img/1.jpg)
- [scans 2](/img/2.jpg)
- [scans 3](/img/3.jpg)
- [scans 4](/img/4.jpg)
- [scans 5](/img/5.jpg)

# subarray architecture

## from fulcrum paper: 
- dram details:
    - interleaving:
        - subarray interleaving:

        The second type of interleaving is subarray interleaving or
        open-bitline architecture . Since the size of a sense
        amplifier is larger than a cell , modern DRAM designs
        accommodate only as many as sense amplifiers in a row to
        sense half a row of cells. To sense the entire row of cells,
        each subarray has bitlines that connect two rows of sense
        amplifiers, one above and one below the subarray.

        As a side benefit, mat interleaving and subarray interleaving make the memory more robust against multiple-bit upset,
        where soft errors change the value of adjacent cells. **In fact, when bits in a column are not physically close to each other, multiple-bit upset only changes one bit from a column and then error detection mechanisms (which can detect one error) can detect the error.**
        Therefore, keeping the current interleaving and not changing the layout is desirable.
        However, with interleaving, row-wide computation on
        more than 4-bit values is impractical, as the result of an addition and multiplication in each 4 bits of the output depends
        on the values in other mats. With row-wide operations, the
        circuits for reuniting the interleaved bits impose a significant
        hardware overhead as many wires cross each other.

        - mat interleaving:

        ![interleaving](/img/interleaving.png)
- fulcrum design:
    - reuniting interleaved bits:

    ![reuniting 1](/img/reuniting.png)


    - the walkers:

    Hence, we introduced
    column-selection latches where we store the one-hot-encoded
    value of a column and shift the value to access the next col-
    umn, without requiring one column decoder per Walker, per
    subarray, and per mat.

    - the controller:
        - a 6-bit counter per Walker
    for detecting a fully-accessed Walker 
        - Each Walker has a 2-bit latch that determines to which Walker
        we should switch and rename when the Walker is fully ac-
        cessed (elaborated in Section 5.1.6), 
        - a 4-bit counter for counting
        the wait time for a new row to be read from the subarray and
        be stored in a Walker, or for a Walker to be written to the
        subarray, 
        - three 11-bit (11 = log 2 of the number of
        rows in each pair of subarrays (2048)) row counters which are
        initialized to the row address of the beginning of the data and
        will be compared against the end of the data in the subarray.

## gearbox architechture:


### SPMSPV walk through:
- step 1: FrontierDistribution:

> In iterative applications, the frontier is generated in previous iterations and already resides in subarrays in which their
corresponding columns reside, except for the output entries that
correspond to long row/columns, which reside in the logic layer. At
the start of each iteration, we broadcast the entries residing in the
logic layer to all subarrays and append them to the frontier array
in each subarray.

the frontier is generated by shared frontier in logic layer and the local frontier.

- step 2: offset packing:
> This step packs the column offset, col-
umn length, and the values from the frontier array that should be
multiplied in the column into a new array. Figure 10 shows the
pseudo-code of this step.

![packing](/img/packing.png)

put the index, lengh and value into a new array.

- step3: localAccumulations:
> This step multiplies each value of
the frontier with its corresponding column. Figure 11 demonstrates
the pseudo-code of this step. In this step, if a clean value is being
updated, the clean value indicator and its row index will be sent to
the Dispatcher.

![localAccumulations](/img/local.png)
- step4: Dispatching:
> In this step, the Dispatcher sends all the
stored entries (index-value pairs) to their destination subarrays.
Here, the Dispatcherâ€™s Walker acts as a buffer.
- step5: RemoteAccumulations:
> In this step, the SPU sequentially processes index-value pairs received in the previous step and
performs the accumulations. Also, in this step, if the value in the
index-value pair is a clean-value indicator, the index of clean-value
is appended to the corresponding array
- step6: applying:
> This step processes the array containing the non-zero indexes to generate the frontier for the next iteration, initializes
the output vector to clean indicators, and sends long-activating
entries to the logic layer to be reduced and applied there. It also
performs the apply operation (ğ‘“ ğ‘–ğ‘›ğ‘ğ‘™ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ [:] = ğ‘‚ğ‘¢ğ‘¡ğ‘ğ‘¢ğ‘¡ [:] + ğ›¼ğ‘¦ [:],
which is explained in Section 2.

## summary of the gearbox:
- the frontier: $$F[0]$$ is the input index, $$F[1]$$ is the input value ,for each frontier, get the pack, $$P[0]$$ is the matrix b index, $$P[1]$$ is the matrix b len, $$P[2]$$ is the value of the frontier.




- local: for each value in P, the P[0] is used as offset to find the correct row, store F_value, len, offset in PE. go throught the row, for each element in the row CSC_pair[0] is row index, CSC_pair[1] is the value, Output[RowId] += F_value * Row_value.



## problems of the gearbox:
- handle dense output write: not mentioned in paper
  -  possible solution one: every time a random access arrived, use the decoder to generage a one-hot value and write the dense output
  -  possible solution two: from the current one-hot value, shift left or right to generate the next one-hot value, and write the dense output 
- handle package orgainzation in dispatcher: not mentioned in paper
  - during the dispatching stage, the dispatcher will send and receive the data. what's the code in the pe to handle sending and receiving data simultaneously?
- icnt and tsv details(port, route): not mentioned in paper
  - the control flow mechanism?
  - there are multiple subarrays sending and multiple subarrays receiving(80000), how the subarrays know that all other subarrays have finished sending and receiving?

# the designe of async spmspm(our work)

## motivation:
- the bottlenech for spmspm of each task is not banlance, if no async, each task's time will be decided by the slowest subarray inside that task. when async is enabled, the time will be decide by slowest subarray with accumulated time.

## description of the design:
to support async remote dispatcher, there are several challenges:
- when sending the remote traffic, all local subarray should be stoped because the dispatcher will be in sending mode and cannot serve the local traffic.
- when receiving the remove traffic, the local subarray should be stoped because the dispatcher will be in receiving mode and cannot serve the local traffic.
- when update the dense result, the local subarray ???
- we should send the remote traffic to a buffer for reducing interupts to the working subarray.
- when start to send the traffic to remote, there should be a flow-control mechanism to avoid the deadlock.
- the buffer management should be carefully designed to save space.

## designs:
- when local dispatcher find TODO number of traffic, it will start to send it to the buffer inside this die
- then the die buffer find TODO number of traffic for some destination, it will start to send it to the remote die
- when the remote die recieved enough traffic, it will send a signal of some bank, make the bank to become recieving mode. and send all traffic to the bank.
- before start sending eveything, the subarray, die buffer will need to acuire a lock first to ensure that there will be no deadlock in ring and the buffer will not be full.

### the buffer design:
- todo: how to partition the buffer, the buffer should be worked as a queue, there are two choice:
    - store all traffic in one unified buffer, when the traffic is full, stop recieving traffic, scan the packages and decide which package are going to be send to the remote: problem: if only a part of the traffic will be able to send. how to fix the hole in the queue.
    - store the traffic in several buffers, each buffer is a queue for a dedicated destination. problem: how to partition the space. if all traffic is of one channel, then the buffer will be underutilized. 
- todo: when to trigger buffer sending. problem: if the buffer of the source die is full, the traffic may not able to be send because there are no enough space in the remote die. two choice: the remote die will send the final update as soon as possible. or there should be a communication mechanism between the source die and the remote die to ensure that the remote die will have enough space to store the traffic.
- todo: how to manage the icnt traffic flow control: [link](http://utenti.dieei.unict.it/users/gascia/COURSES/sist_emb_14_15/download/SE22_noc_flow_control.pdf)

### the buffer in logic die:
- partition the buffer:
  - the traffic coming from the source die will be forwarded to the target die, target bank, if we should partition the buffer into servral parts"
    - per bank
    - per die
  - do not partition the buffer, use the unified buffer
- TODO, how to trigger sending

### the package sending design of source subarray:
the dispatcher will send the traffic.

#### problem:
- when the dispatcher is recieving traffic, there should be 3 types of traffic:
    1. same bank
    2. different bank, same die
    3. different die
- when handle the traffic, if simply send all traffic to the buffer. the size of the queue may be too large, because the type 1,2 not need to be there.

#### TODO solution:
- (TODO) solution1: when the dispatcher is full, directly send the traffic to the local bank, but send the traffic to the layer buffer for type 2,3
- (TODO) solution2: when the dispatcher is full, scan the packages, send signal to the banks that should receive the traffic, and make the banks ready. directly send type2 to bank, only send type3 to the buffer
- (TOOD) solution3: send all traffic to the buffer. only send the traffic when the buffer is triggered to send.

### the package update design of destination subarray:
the subarray will recieve the remote traffic.

#### problem:
- when recieve the traffic, should the bank immediatly update the destination?
    - if yes, all subarray should be stoped and the walkers may need to be reset to be ready to write.
    - if no, (TODO) seems no benifit because the subarray still need to stop.

#### TODO solution:
- (TODO), because the gearbox paper do not have details of dense write. so we do not know how to do it.

## miscs:
- (TODO) how to handle and when to transfore the dense formate to sparse formate.
- (TODO) When perform the transfomation, the subarray should be stoped, and the walker should be reset to be ready to write.